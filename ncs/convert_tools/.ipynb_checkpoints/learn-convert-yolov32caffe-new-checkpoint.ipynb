{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime  \n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])\n",
    "            '''\n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            '''\n",
    "            layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark\n",
      "[('layer1-conv', [(16, 3, 3, 3)]), ('layer1-bn', [(16,), (16,), (1,)]), ('layer1-scale', [(16,), (16,)]), ('layer3-conv', [(32, 16, 3, 3)]), ('layer3-bn', [(32,), (32,), (1,)]), ('layer3-scale', [(32,), (32,)]), ('layer5-conv', [(64, 32, 3, 3)]), ('layer5-bn', [(64,), (64,), (1,)]), ('layer5-scale', [(64,), (64,)]), ('layer7-conv', [(128, 64, 3, 3)]), ('layer7-bn', [(128,), (128,), (1,)]), ('layer7-scale', [(128,), (128,)]), ('layer9-conv', [(256, 128, 3, 3)]), ('layer9-bn', [(256,), (256,), (1,)]), ('layer9-scale', [(256,), (256,)]), ('layer11-conv', [(512, 256, 3, 3)]), ('layer11-bn', [(512,), (512,), (1,)]), ('layer11-scale', [(512,), (512,)]), ('layer12-conv', [(1024, 512, 3, 3)]), ('layer12-bn', [(1024,), (1024,), (1,)]), ('layer12-scale', [(1024,), (1024,)]), ('layer13-conv', [(256, 1024, 1, 1)]), ('layer13-bn', [(256,), (256,), (1,)]), ('layer13-scale', [(256,), (256,)]), ('layer14-conv', [(512, 256, 3, 3)]), ('layer14-bn', [(512,), (512,), (1,)]), ('layer14-scale', [(512,), (512,)]), ('layer15-conv', [(33, 512, 1, 1), (33,)]), ('layer18-conv', [(128, 256, 1, 1)]), ('layer18-bn', [(128,), (128,), (1,)]), ('layer18-scale', [(128,), (128,)]), ('layer19-conv', [(128, 128, 2, 2), (128,)]), ('layer21-conv', [(256, 384, 3, 3)]), ('layer21-bn', [(256,), (256,), (1,)]), ('layer21-scale', [(256,), (256,)]), ('layer22-conv', [(33, 256, 1, 1), (33,)])]\n",
      "benchmark\n",
      "benchmark\n",
      "['net_1', 'convolutional_2', 'maxpool_3', 'convolutional_4', 'maxpool_5', 'convolutional_6', 'maxpool_7', 'convolutional_8', 'maxpool_9', 'convolutional_10', 'maxpool_11', 'convolutional_12', 'convolutional_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'yolo_17', 'route_18', 'convolutional_19', 'deconvolutional_20', 'route_21', 'convolutional_22', 'convolutional_23', 'yolo_24']\n",
      "[      0       2       0 7180800       0]\n",
      "buf len:8753458\n",
      "net_1\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.001', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '500200', 'steps': '400000,450000', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "convolutional_2\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer1-conv\n",
      "load_conv_bn2caffe:\n",
      "start:496\n",
      "maxpool_3\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_4\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer3-conv\n",
      "load_conv_bn2caffe:\n",
      "start:5232\n",
      "maxpool_5\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_6\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer5-conv\n",
      "load_conv_bn2caffe:\n",
      "start:23920\n",
      "maxpool_7\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_8\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer7-conv\n",
      "load_conv_bn2caffe:\n",
      "start:98160\n",
      "maxpool_9\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_10\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer9-conv\n",
      "load_conv_bn2caffe:\n",
      "start:394096\n",
      "maxpool_11\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_12\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer11-conv\n",
      "load_conv_bn2caffe:\n",
      "start:1575792\n",
      "convolutional_13\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer12-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6298480\n",
      "convolutional_14\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer13-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6561648\n",
      "convolutional_15\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer14-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7743344\n",
      "convolutional_16\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '33', 'size': '1'}\n",
      "has no name layer15-conv\n",
      "load_conv2caffe:\n",
      "start:7760273\n",
      "yolo_17\n",
      "{'jitter': '.3', 'anchors': '10,14,  23,27,  37,58,  81,82,  135,169,  344,319', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '6', 'ignore_thresh': '.7', 'truth_thresh': '1'}\n",
      "unknow layer type yolo \n",
      "route_18\n",
      "{'layers': '-4'}\n",
      "convolutional_19\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer18-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7793553\n",
      "deconvolutional_20\n",
      "{'stride': '2', 'activation': 'leaky', 'pad': '0', 'filters': '128', 'size': '2'}\n",
      "has no name layer19-upsample\n",
      "load_conv2caffe:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layer19-upsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d16bcf42b51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#caffemodel = 'Jenerated_nolastpooling.caffemodel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcaffemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d%H%M%S_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'TinyYoloV3NCS.caffemodel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdarknet2caffe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffemodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-852c0335ebfd>\u001b[0m in \u001b[0;36mdarknet2caffe\u001b[0;34m(cfgfile, weightfile, protofile, caffemodel)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_conv2caffe:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_conv2caffe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv_layer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             '''\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layer19-upsample'"
     ]
    }
   ],
   "source": [
    "cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "weights = '/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_112100.backup'\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/18:44:10.855133yolov3-tiny-ncs-without-last-maxpool.cfg.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '', '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools', '/data/ssd-caffe/caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/yyp/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/home/yyp/.local/lib/python2.7/site-packages/IPython/extensions', '/home/yyp/.ipython']\n",
      "['net_1', 'convolutional_2', 'convolutional_3', 'convolutional_4', 'convolutional_5', 'convolutional_6', 'convolutional_7', 'convolutional_8', 'convolutional_9', 'convolutional_10', 'convolutional_11', 'yolo_12', 'route_13', 'convolutional_14', 'deconvolutional_15', 'route_16', 'convolutional_17', 'convolutional_18', 'yolo_19']\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.0005', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '200200', 'steps': '400000,450000', 'type': 'net', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '24', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,25,  20,50,  30,75, 50,125,  80,200,  150,150', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '3', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n",
      "{'layers': '-4', 'type': 'route'}\n",
      "-4\n",
      "OrderedDict([('bottom', 'layer8-conv'), ('top', 'layer12-route'), ('name', 'layer12-route'), ('type', 'Concat')])\n",
      "12\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '0', 'filters': '128', 'type': 'deconvolutional', 'size': '2'}\n",
      "{'layers': '-1, 3', 'type': 'route'}\n",
      "-1\n",
      "{1: 'layer1-conv', 2: 'layer2-conv', 3: 'layer3-conv', 4: 'layer4-conv', 5: 'layer5-conv', 6: 'layer6-conv', 7: 'layer7-conv', 8: 'layer8-conv', 9: 'layer9-conv', 10: 'layer10-conv', 12: 'layer12-route', 13: 'layer13-conv', 14: 'layer14-upsample'}\n",
      "OrderedDict([('bottom', ['layer14-upsample', 'layer4-conv']), ('top', 'layer15-route'), ('name', 'layer15-route'), ('type', 'Concat')])\n",
      "15\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '24', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,25,  20,50,  30,75, 50,125,  80,200,  150,150', 'random': '1', 'mask': '0,1,2', 'num': '6', 'classes': '3', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0,'/data/ssd-caffe/py2_caffe/python/')\n",
    "import caffe  \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "class UniqDict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def parse_cfg(cfgfile):\n",
    "    parser = ConfigParser(dict_type=UniqDict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    return blocks, parser\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def cfg2prototxt(cfgfile):\n",
    "    blocks, parser = parse_cfg(cfgfile)\n",
    "    print(blocks)\n",
    "    layers = []  \n",
    "    props = OrderedDict()  \n",
    "    bottom = 'data'  \n",
    "    layer_id = 1  \n",
    "    topnames = dict()  \n",
    "    for block in blocks:\n",
    "        items = dict(parser.items(block))\n",
    "        items['type'] = block.split('_')[0]\n",
    "        print(items)\n",
    "        if items['type'] == 'net':  \n",
    "            props['name'] = 'Darkent2Caffe'  \n",
    "            props['input'] = 'data'  \n",
    "            props['input_dim'] = ['1']  \n",
    "            props['input_dim'].append(items['channels'])  \n",
    "            props['input_dim'].append(items['height'])  \n",
    "            props['input_dim'].append(items['width'])  \n",
    "            continue  \n",
    "        elif (items['type'] == 'convolutional' or items['type'] == 'deconvolutional'):  \n",
    "            conv_layer = OrderedDict()  \n",
    "            conv_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer['top'] = items['name']  \n",
    "                conv_layer['name'] = items['name']  \n",
    "            else:\n",
    "                if(items['type'] == 'convolutional'):\n",
    "                    conv_layer['top'] = 'layer%d-conv' % layer_id  \n",
    "                    conv_layer['name'] = 'layer%d-conv' % layer_id\n",
    "                elif(items['type'] == 'deconvolutional'):\n",
    "                    conv_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                    conv_layer['name'] = 'layer%d-upsample' % layer_id\n",
    "                    \n",
    "            if(items['type'] == 'deconvolutional'):\n",
    "                conv_layer['type'] = 'Deconvolution'\n",
    "            elif(items['type'] == 'convolutional'):\n",
    "                conv_layer['type'] = 'Convolution' \n",
    "            convolution_param = OrderedDict()  \n",
    "            convolution_param['num_output'] = items['filters']  \n",
    "            convolution_param['kernel_size'] = items['size']  \n",
    "            if items['pad'] == '1':  \n",
    "                convolution_param['pad'] = str(int(convolution_param['kernel_size'])/2)  \n",
    "            convolution_param['stride'] = items['stride']\n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1':  \n",
    "                    convolution_param['bias_term'] = 'false' \n",
    "            else:  \n",
    "                convolution_param['bias_term'] = 'true'  \n",
    "            conv_layer['convolution_param'] = convolution_param  \n",
    "            layers.append(conv_layer)  \n",
    "            bottom = conv_layer['top']  \n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1': \n",
    "                    bn_layer = OrderedDict()  \n",
    "                    bn_layer['bottom'] = bottom  \n",
    "                    bn_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        bn_layer['name'] = '%s-bn' % items['name']  \n",
    "                    else:  \n",
    "                        bn_layer['name'] = 'layer%d-bn' % layer_id  \n",
    "                    bn_layer['type'] = 'BatchNorm'  \n",
    "                    batch_norm_param = OrderedDict()  \n",
    "                    batch_norm_param['use_global_stats'] = 'true'  \n",
    "                    bn_layer['batch_norm_param'] = batch_norm_param  \n",
    "                    layers.append(bn_layer)  \n",
    "\n",
    "                    scale_layer = OrderedDict()  \n",
    "                    scale_layer['bottom'] = bottom  \n",
    "                    scale_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        scale_layer['name'] = '%s-scale' % items['name']  \n",
    "                    else:  \n",
    "                        scale_layer['name'] = 'layer%d-scale' % layer_id  \n",
    "                    scale_layer['type'] = 'Scale'  \n",
    "                    scale_param = OrderedDict()  \n",
    "                    scale_param['bias_term'] = 'true'  \n",
    "                    scale_layer['scale_param'] = scale_param  \n",
    "                    layers.append(scale_layer)  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'maxpool':  \n",
    "            max_layer = OrderedDict()  \n",
    "            max_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                max_layer['top'] = items['name']  \n",
    "                max_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                max_layer['top'] = 'layer%d-maxpool' % layer_id  \n",
    "                max_layer['name'] = 'layer%d-maxpool' % layer_id  \n",
    "            max_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = items['size']  \n",
    "            pooling_param['stride'] = items['stride']  \n",
    "            pooling_param['pool'] = 'MAX'  \n",
    "            if items.has_key('pad') and int(items['pad']) == 1:  \n",
    "                pooling_param['pad'] = str((int(items['size'])-1)/2)  \n",
    "            max_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(max_layer)  \n",
    "            bottom = max_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'avgpool':  \n",
    "            avg_layer = OrderedDict()  \n",
    "            avg_layer['bottom'] = bottom  \n",
    "            if block.has_key('name'):  \n",
    "                avg_layer['top'] = items['name']  \n",
    "                avg_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                avg_layer['top'] = 'layer%d-avgpool' % layer_id  \n",
    "                avg_layer['name'] = 'layer%d-avgpool' % layer_id  \n",
    "            avg_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = 7  \n",
    "            pooling_param['stride'] = 1  \n",
    "            pooling_param['pool'] = 'AVE'  \n",
    "            avg_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(avg_layer)  \n",
    "            bottom = avg_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'yolo': \n",
    "            layer_id = layer_id + 1\n",
    "            continue\n",
    "            region_layer = OrderedDict()  \n",
    "            region_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                region_layer['top'] = items['name']  \n",
    "                region_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                region_layer['top'] = 'layer%d-yolo' % layer_id  \n",
    "                region_layer['name'] = 'layer%d-yolo' % layer_id  \n",
    "            region_layer['type'] = 'Yolo'  \n",
    "            region_param = OrderedDict()  \n",
    "            region_param['anchors'] = items['anchors'].strip()  \n",
    "            region_param['classes'] = items['classes']  \n",
    "            region_param['num'] = items['num']  \n",
    "            region_layer['yolo_param'] = region_param  \n",
    "            layers.append(region_layer)  \n",
    "            bottom = region_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1\n",
    "\n",
    "        elif items['type'] == 'route':\n",
    "            route_layer = OrderedDict()  \n",
    "            layer_name = str(items['layers']).split(',')  \n",
    "            print(layer_name[0])  \n",
    "            bottom_layer_size = len(str(items['layers']).split(','))  \n",
    "        #print(bottom_layer_size)  \n",
    "            if(1 == bottom_layer_size):  \n",
    "                prev_layer_id = layer_id + int(items['layers'])  \n",
    "                bottom = topnames[prev_layer_id]  \n",
    "                #topnames[layer_id] = bottom  \n",
    "            route_layer['bottom'] = bottom  \n",
    "            if(2 == bottom_layer_size):  \n",
    "                prev_layer_id1 = layer_id + int(layer_name[0])  \n",
    "                #print(prev_layer_id1)  \n",
    "                prev_layer_id2 = int(layer_name[1]) + 1  \n",
    "                print(topnames)  \n",
    "                bottom1 = topnames[prev_layer_id1]  \n",
    "                bottom2 = topnames[prev_layer_id2]  \n",
    "                route_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                route_layer['top'] = items['name']  \n",
    "                route_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                route_layer['top'] = 'layer%d-route' % layer_id  \n",
    "                route_layer['name'] = 'layer%d-route' % layer_id  \n",
    "            route_layer['type'] = 'Concat'\n",
    "            print(route_layer)  \n",
    "            layers.append(route_layer)  \n",
    "            bottom = route_layer['top']  \n",
    "            print(layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'upsample':\n",
    "            upsample_layer = OrderedDict()  \n",
    "            print(items['stride'])  \n",
    "            upsample_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                upsample_layer['top'] = items['name']  \n",
    "                upsample_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                upsample_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                upsample_layer['name'] = 'layer%d-upsample' % layer_id  \n",
    "            upsample_layer['type'] = 'Upsample'  \n",
    "            upsample_param = OrderedDict()  \n",
    "            upsample_param['scale'] = items['stride']  \n",
    "            upsample_layer['upsample_param'] = upsample_param  \n",
    "            print(upsample_layer)  \n",
    "            layers.append(upsample_layer)  \n",
    "            bottom = upsample_layer['top']  \n",
    "            print('upsample:',layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'shortcut':  \n",
    "            prev_layer_id1 = layer_id + int(items['from'])  \n",
    "            prev_layer_id2 = layer_id - 1  \n",
    "            bottom1 = topnames[prev_layer_id1]  \n",
    "            bottom2= topnames[prev_layer_id2]  \n",
    "            shortcut_layer = OrderedDict()  \n",
    "            shortcut_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                shortcut_layer['top'] = items['name']  \n",
    "                shortcut_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                shortcut_layer['top'] = 'layer%d-shortcut' % layer_id  \n",
    "                shortcut_layer['name'] = 'layer%d-shortcut' % layer_id  \n",
    "            shortcut_layer['type'] = 'Eltwise'\n",
    "            eltwise_param = OrderedDict()  \n",
    "            eltwise_param['operation'] = 'SUM'  \n",
    "            shortcut_layer['eltwise_param'] = eltwise_param  \n",
    "            layers.append(shortcut_layer)  \n",
    "            bottom = shortcut_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if block.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1             \n",
    "\n",
    "        elif items['type'] == 'connected':  \n",
    "            fc_layer = OrderedDict()  \n",
    "            fc_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer['top'] = items['name']  \n",
    "                fc_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                fc_layer['top'] = 'layer%d-fc' % layer_id  \n",
    "                fc_layer['name'] = 'layer%d-fc' % layer_id  \n",
    "            fc_layer['type'] = 'InnerProduct'  \n",
    "            fc_param = OrderedDict()  \n",
    "            fc_param['num_output'] = int(items['output'])  \n",
    "            fc_layer['inner_product_param'] = fc_param  \n",
    "            layers.append(fc_layer)  \n",
    "            bottom = fc_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % items['type'])  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "    net_info = OrderedDict()  \n",
    "    net_info['props'] = props  \n",
    "    net_info['layers'] = layers  \n",
    "    return net_info\n",
    "\n",
    "def save_prototxt(net_info, protofile, yolo=False):\n",
    "    fp = open(protofile, 'w')\n",
    "    # whether add double quote\n",
    "    def format_value(value):\n",
    "        #str = u'%s' % value\n",
    "        #if str.isnumeric():\n",
    "        if is_number(value):\n",
    "            return value\n",
    "        elif value == 'true' or value == 'false' or value == 'MAX' or value == 'SUM' or value == 'AVE':\n",
    "            return value\n",
    "        else:\n",
    "            return '\\\"%s\\\"' % value\n",
    "\n",
    "    def print_block(block_info, prefix, indent):\n",
    "        blanks = ''.join([' ']*indent)\n",
    "        print >>fp, '%s%s {' % (blanks, prefix)\n",
    "        for key,value in block_info.items():\n",
    "            if type(value) == OrderedDict:\n",
    "                print_block(value, key, indent+4)\n",
    "            elif type(value) == list:\n",
    "                for v in value:\n",
    "                    print >> fp, '%s    %s: %s' % (blanks, key, format_value(v))\n",
    "            else:\n",
    "                print >> fp, '%s    %s: %s' % (blanks, key, format_value(value))\n",
    "        print >> fp, '%s}' % blanks\n",
    "        \n",
    "    props = net_info['props']\n",
    "    layers = net_info['layers']\n",
    "    print >> fp, 'name: \\\"%s\\\"' % props['name']\n",
    "    print >> fp, 'input: \\\"%s\\\"' % props['input']\n",
    "    print >> fp, 'input_shape{'\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][0]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][1]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][2]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][3]\n",
    "    print >> fp, '}'\n",
    "    #print >> fp, ''\n",
    "    for layer in layers:\n",
    "        if layer['type'] != 'yolo' or yolo == True:\n",
    "            print_block(layer, 'layer', 0)\n",
    "    fp.close()\n",
    "cfgfile = '/data/github_repos/darknet/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "cfgfile = '/data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-remove-maxpooling.cfg'\n",
    "saved_prototxt = str(datetime.datetime.now()).split(' ')[-1] + cfgfile.split('/')[-1] + '.prototxt'\n",
    "net_info = cfg2prototxt(cfgfile)    \n",
    "save_prototxt(net_info, saved_prototxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark\n",
      "[('layer1-conv', [(16, 3, 3, 3)]), ('layer1-bn', [(16,), (16,), (1,)]), ('layer1-scale', [(16,), (16,)]), ('layer2-conv', [(32, 16, 3, 3)]), ('layer2-bn', [(32,), (32,), (1,)]), ('layer2-scale', [(32,), (32,)]), ('layer3-conv', [(64, 32, 3, 3)]), ('layer3-bn', [(64,), (64,), (1,)]), ('layer3-scale', [(64,), (64,)]), ('layer4-conv', [(128, 64, 3, 3)]), ('layer4-bn', [(128,), (128,), (1,)]), ('layer4-scale', [(128,), (128,)]), ('layer5-conv', [(256, 128, 3, 3)]), ('layer5-bn', [(256,), (256,), (1,)]), ('layer5-scale', [(256,), (256,)]), ('layer6-conv', [(512, 256, 3, 3)]), ('layer6-bn', [(512,), (512,), (1,)]), ('layer6-scale', [(512,), (512,)]), ('layer7-conv', [(1024, 512, 3, 3)]), ('layer7-bn', [(1024,), (1024,), (1,)]), ('layer7-scale', [(1024,), (1024,)]), ('layer8-conv', [(256, 1024, 1, 1)]), ('layer8-bn', [(256,), (256,), (1,)]), ('layer8-scale', [(256,), (256,)]), ('layer9-conv', [(512, 256, 3, 3)]), ('layer9-bn', [(512,), (512,), (1,)]), ('layer9-scale', [(512,), (512,)]), ('layer10-conv', [(24, 512, 1, 1), (24,)]), ('layer13-conv', [(128, 256, 1, 1)]), ('layer13-bn', [(128,), (128,), (1,)]), ('layer13-scale', [(128,), (128,)]), ('layer14-upsample', [(128, 128, 2, 2), (128,)]), ('layer16-conv', [(256, 256, 3, 3)]), ('layer16-bn', [(256,), (256,), (1,)]), ('layer16-scale', [(256,), (256,)]), ('layer17-conv', [(24, 256, 1, 1), (24,)])]\n",
      "benchmark\n",
      "benchmark\n"
     ]
    },
    {
     "ename": "MissingSectionHeaderError",
     "evalue": "File contains no section headers.\nfile: /data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-remove-maxpooling.weights, line: 1\n'\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00L\\x04\\x00\\x00\\x00\\x00\\x00;A\\x8c\\xbe5i\\xc2\\xbdG\\xe7\\xe5:\\x14~X\\xbd\\xe3\\x08\\xdd\\xbb\\x88\\xd6\\xaa\\xbdW\\x05\"\\xbf\\xc4<\\xc1\\xbd;\\x8f}\\xbe(\\xc5\\x97\\xbe\\xc5?\\xb4=\\xb4\\xe2S\\xbe\\xc6\\x83\\x83>]\\x8b\\xfa\\xbe\\xd8\\x8be>!\\xdd\\x12=\\x8ek\\xa3?\\xc7\\x85B?\\xb2B_?\\xad\\xc8\\x8b?<\\x18A?\\xbc\\xad\\x8d?\\xc6)\\\\?m\\xdfM?\\xffjt?\\x0c\\xc8\\xba?LrS?{\\xb4\\xb0?W\\tf?\\xd6\\xeeg?\\x9f\\x995?\\x1b\\xe2L?\\'!\\xb8\\xbdKW\\x86?\\xc9\\x93\\x1a>,\\xcf\\xa4\\xbe\"I\\x0c?\"\\x17$\\xbf\\xd0\\xdb\\xb6\\xbe\\xb4\\xf0\\xb1\\xbe\\xe4\\xf0\\xd3\\xbd\\x9b\\xe9\\x88\\xbd\\x87\\x8e\\xcb>\\xad\\xd4\\r\\xbd\\x0c\\xf44>\\x07#\\x9e\\xbf\\x90\\xd1\\x1a\\xbem6\\'>\\xce\\xb7\\xb8=\\x0fHD>\\x84\\x0b\\xa4<\\xbc\\xa8J=\\xbe\\x85\\x94=?\\x1e\\x8f=\\x83\\x93J=\\xe1\\x8f\\x1c=\\x7f\\xff\\x88=;\\xd3\\xf0=\\x1d)I=k\\x96O=\\x92m\\xf2<\\xc6\\xe6\\x91>F\\x8e\\xe2<p\\xad\\xa0<\\xea\\x84N>@.\\x16?\\xd5\\x14<?\\'kz\\xbeL\\xe5p\\xbf\\x83\\xa6\\x10\\xbe\\xcccJ\\xbd\\x02\\x07r\\xbe\\x16\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSectionHeaderError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6d57a92b450b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m#caffemodel = 'Jenerated_nolastpooling.caffemodel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0mcaffemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d%H%M%S_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'TinyYoloV3NCS.caffemodel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0mdarknet2caffe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffemodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0mmake_prototxt_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cp '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprototxt\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcaffemodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'prototxt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0mprototxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffemodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'prototxt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-6d57a92b450b>\u001b[0m in \u001b[0;36mdarknet2caffe\u001b[0;34m(cfgfile, weightfile, protofile, caffemodel)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#blocks = parse_cfg(cfgfile)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniqdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ConfigParser.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, filenames)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mread_ok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ConfigParser.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, fp, fpname)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;31m# no section header in the file?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcursect\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mMissingSectionHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# an option line?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSectionHeaderError\u001b[0m: File contains no section headers.\nfile: /data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-remove-maxpooling.weights, line: 1\n'\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00L\\x04\\x00\\x00\\x00\\x00\\x00;A\\x8c\\xbe5i\\xc2\\xbdG\\xe7\\xe5:\\x14~X\\xbd\\xe3\\x08\\xdd\\xbb\\x88\\xd6\\xaa\\xbdW\\x05\"\\xbf\\xc4<\\xc1\\xbd;\\x8f}\\xbe(\\xc5\\x97\\xbe\\xc5?\\xb4=\\xb4\\xe2S\\xbe\\xc6\\x83\\x83>]\\x8b\\xfa\\xbe\\xd8\\x8be>!\\xdd\\x12=\\x8ek\\xa3?\\xc7\\x85B?\\xb2B_?\\xad\\xc8\\x8b?<\\x18A?\\xbc\\xad\\x8d?\\xc6)\\\\?m\\xdfM?\\xffjt?\\x0c\\xc8\\xba?LrS?{\\xb4\\xb0?W\\tf?\\xd6\\xeeg?\\x9f\\x995?\\x1b\\xe2L?\\'!\\xb8\\xbdKW\\x86?\\xc9\\x93\\x1a>,\\xcf\\xa4\\xbe\"I\\x0c?\"\\x17$\\xbf\\xd0\\xdb\\xb6\\xbe\\xb4\\xf0\\xb1\\xbe\\xe4\\xf0\\xd3\\xbd\\x9b\\xe9\\x88\\xbd\\x87\\x8e\\xcb>\\xad\\xd4\\r\\xbd\\x0c\\xf44>\\x07#\\x9e\\xbf\\x90\\xd1\\x1a\\xbem6\\'>\\xce\\xb7\\xb8=\\x0fHD>\\x84\\x0b\\xa4<\\xbc\\xa8J=\\xbe\\x85\\x94=?\\x1e\\x8f=\\x83\\x93J=\\xe1\\x8f\\x1c=\\x7f\\xff\\x88=;\\xd3\\xf0=\\x1d)I=k\\x96O=\\x92m\\xf2<\\xc6\\xe6\\x91>F\\x8e\\xe2<p\\xad\\xa0<\\xea\\x84N>@.\\x16?\\xd5\\x14<?\\'kz\\xbeL\\xe5p\\xbf\\x83\\xa6\\x10\\xbe\\xcccJ\\xbd\\x02\\x07r\\xbe\\x16\\n'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "if(len(sys.argv) < 2):\n",
    "    print('please input yolov3-tiny-ncs-weights file')\n",
    "    exit(1)\n",
    "weights = sys.argv[1]\n",
    "'''\n",
    "weights = '/data/darknet/backup/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_140000.weights'\n",
    "weights = '/data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-remove-maxpooling.weights'\n",
    "\n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "del caffe\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])\n",
    "            '''\n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            '''\n",
    "            layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)\n",
    "    \n",
    "#cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "#weights = weights\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "prototxt = saved_prototxt\n",
    "prototxt1 = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/07:30:49.817376yolov3-tiny-ncs-without-last-maxpool.cfg.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)\n",
    "make_prototxt_command = 'cp ' + prototxt +' ' + caffemodel[:-10] + 'prototxt'\n",
    "prototxt = caffemodel[:-10] + 'prototxt'\n",
    "os.system(make_prototxt_command)\n",
    "#mvnc_command = 'bash generate-graph.sh ' + caffemodel + ' ' + prototxt1\n",
    "#os.system(mvnc_command)\n",
    "set_ncs2_env_command = 'bash /opt/intel/computer_vision_sdk/bin/setupvars.sh'\n",
    "os.system(set_ncs2_env_command)\n",
    "mo_command = 'sh /data/github_repos/yolov3-tiny-fit-ncs/ncs2/OpenVINO/model_optimizer/yolov3-tiny-mo.sh ' + '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/' + caffemodel\n",
    "os.system(mo_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
