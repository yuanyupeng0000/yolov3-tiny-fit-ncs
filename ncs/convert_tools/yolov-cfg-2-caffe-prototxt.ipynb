{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '', '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools', '/data/ssd-caffe/caffe/python', '/data/ssd-caffe/new-yolov3-caffe/python', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/yyp/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/home/yyp/.local/lib/python2.7/site-packages/IPython/extensions', '/home/yyp/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0,'/data/ssd-caffe/py2_caffe/python/')\n",
    "import caffe  \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqDict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfg(cfgfile):\n",
    "    parser = ConfigParser(dict_type=UniqDict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    return blocks, parser\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg2prototxt(cfgfile):\n",
    "    blocks, parser = parse_cfg(cfgfile)\n",
    "    print(blocks)\n",
    "    layers = []  \n",
    "    props = OrderedDict()  \n",
    "    bottom = 'data'  \n",
    "    layer_id = 1  \n",
    "    topnames = dict()  \n",
    "    for block in blocks:\n",
    "        items = dict(parser.items(block))\n",
    "        items['type'] = block.split('_')[0]\n",
    "        print(items)\n",
    "        if items['type'] == 'net':  \n",
    "            props['name'] = 'Darkent2Caffe'  \n",
    "            props['input'] = 'data'  \n",
    "            props['input_dim'] = ['1']  \n",
    "            props['input_dim'].append(items['channels'])  \n",
    "            props['input_dim'].append(items['height'])  \n",
    "            props['input_dim'].append(items['width'])  \n",
    "            continue  \n",
    "        elif (items['type'] == 'convolutional' or items['type'] == 'deconvolutional'):  \n",
    "            conv_layer = OrderedDict()  \n",
    "            conv_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer['top'] = items['name']  \n",
    "                conv_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                conv_layer['top'] = 'layer%d-conv' % layer_id  \n",
    "                conv_layer['name'] = 'layer%d-conv' % layer_id\n",
    "            if(items['type'] == 'deconvolutional'):\n",
    "                conv_layer['type'] = 'Deconvolution'\n",
    "            elif(items['type'] == 'convolutional'):\n",
    "                conv_layer['type'] = 'Convolution' \n",
    "            convolution_param = OrderedDict()  \n",
    "            convolution_param['num_output'] = items['filters']  \n",
    "            convolution_param['kernel_size'] = items['size']  \n",
    "            if items['pad'] == '1':  \n",
    "                convolution_param['pad'] = str(int(convolution_param['kernel_size'])/2)  \n",
    "            convolution_param['stride'] = items['stride']\n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1':  \n",
    "                    convolution_param['bias_term'] = 'false' \n",
    "            else:  \n",
    "                convolution_param['bias_term'] = 'true'  \n",
    "            conv_layer['convolution_param'] = convolution_param  \n",
    "            layers.append(conv_layer)  \n",
    "            bottom = conv_layer['top']  \n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1': \n",
    "                    bn_layer = OrderedDict()  \n",
    "                    bn_layer['bottom'] = bottom  \n",
    "                    bn_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        bn_layer['name'] = '%s-bn' % items['name']  \n",
    "                    else:  \n",
    "                        bn_layer['name'] = 'layer%d-bn' % layer_id  \n",
    "                    bn_layer['type'] = 'BatchNorm'  \n",
    "                    batch_norm_param = OrderedDict()  \n",
    "                    batch_norm_param['use_global_stats'] = 'true'  \n",
    "                    bn_layer['batch_norm_param'] = batch_norm_param  \n",
    "                    layers.append(bn_layer)  \n",
    "\n",
    "                    scale_layer = OrderedDict()  \n",
    "                    scale_layer['bottom'] = bottom  \n",
    "                    scale_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        scale_layer['name'] = '%s-scale' % items['name']  \n",
    "                    else:  \n",
    "                        scale_layer['name'] = 'layer%d-scale' % layer_id  \n",
    "                    scale_layer['type'] = 'Scale'  \n",
    "                    scale_param = OrderedDict()  \n",
    "                    scale_param['bias_term'] = 'true'  \n",
    "                    scale_layer['scale_param'] = scale_param  \n",
    "                    layers.append(scale_layer)  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'maxpool':  \n",
    "            max_layer = OrderedDict()  \n",
    "            max_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                max_layer['top'] = items['name']  \n",
    "                max_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                max_layer['top'] = 'layer%d-maxpool' % layer_id  \n",
    "                max_layer['name'] = 'layer%d-maxpool' % layer_id  \n",
    "            max_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = items['size']  \n",
    "            pooling_param['stride'] = items['stride']  \n",
    "            pooling_param['pool'] = 'MAX'  \n",
    "            if items.has_key('pad') and int(items['pad']) == 1:  \n",
    "                pooling_param['pad'] = str((int(items['size'])-1)/2)  \n",
    "            max_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(max_layer)  \n",
    "            bottom = max_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'avgpool':  \n",
    "            avg_layer = OrderedDict()  \n",
    "            avg_layer['bottom'] = bottom  \n",
    "            if block.has_key('name'):  \n",
    "                avg_layer['top'] = items['name']  \n",
    "                avg_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                avg_layer['top'] = 'layer%d-avgpool' % layer_id  \n",
    "                avg_layer['name'] = 'layer%d-avgpool' % layer_id  \n",
    "            avg_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = 7  \n",
    "            pooling_param['stride'] = 1  \n",
    "            pooling_param['pool'] = 'AVE'  \n",
    "            avg_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(avg_layer)  \n",
    "            bottom = avg_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'yolo': \n",
    "            layer_id = layer_id + 1\n",
    "            continue\n",
    "            region_layer = OrderedDict()  \n",
    "            region_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                region_layer['top'] = items['name']  \n",
    "                region_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                region_layer['top'] = 'layer%d-yolo' % layer_id  \n",
    "                region_layer['name'] = 'layer%d-yolo' % layer_id  \n",
    "            region_layer['type'] = 'Yolo'  \n",
    "            region_param = OrderedDict()  \n",
    "            region_param['anchors'] = items['anchors'].strip()  \n",
    "            region_param['classes'] = items['classes']  \n",
    "            region_param['num'] = items['num']  \n",
    "            region_layer['yolo_param'] = region_param  \n",
    "            layers.append(region_layer)  \n",
    "            bottom = region_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1\n",
    "\n",
    "        elif items['type'] == 'route':\n",
    "            route_layer = OrderedDict()  \n",
    "            layer_name = str(items['layers']).split(',')  \n",
    "            print(layer_name[0])  \n",
    "            bottom_layer_size = len(str(items['layers']).split(','))  \n",
    "        #print(bottom_layer_size)  \n",
    "            if(1 == bottom_layer_size):  \n",
    "                prev_layer_id = layer_id + int(items['layers'])  \n",
    "                bottom = topnames[prev_layer_id]  \n",
    "                #topnames[layer_id] = bottom  \n",
    "            route_layer['bottom'] = bottom  \n",
    "            if(2 == bottom_layer_size):  \n",
    "                prev_layer_id1 = layer_id + int(layer_name[0])  \n",
    "                #print(prev_layer_id1)  \n",
    "                prev_layer_id2 = int(layer_name[1]) + 1  \n",
    "                print(topnames)  \n",
    "                bottom1 = topnames[prev_layer_id1]  \n",
    "                bottom2 = topnames[prev_layer_id2]  \n",
    "                route_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                route_layer['top'] = items['name']  \n",
    "                route_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                route_layer['top'] = 'layer%d-route' % layer_id  \n",
    "                route_layer['name'] = 'layer%d-route' % layer_id  \n",
    "            route_layer['type'] = 'Concat'\n",
    "            print(route_layer)  \n",
    "            layers.append(route_layer)  \n",
    "            bottom = route_layer['top']  \n",
    "            print(layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'upsample':\n",
    "            upsample_layer = OrderedDict()  \n",
    "            print(items['stride'])  \n",
    "            upsample_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                upsample_layer['top'] = items['name']  \n",
    "                upsample_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                upsample_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                upsample_layer['name'] = 'layer%d-upsample' % layer_id  \n",
    "            upsample_layer['type'] = 'Upsample'  \n",
    "            upsample_param = OrderedDict()  \n",
    "            upsample_param['scale'] = items['stride']  \n",
    "            upsample_layer['upsample_param'] = upsample_param  \n",
    "            print(upsample_layer)  \n",
    "            layers.append(upsample_layer)  \n",
    "            bottom = upsample_layer['top']  \n",
    "            print('upsample:',layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'shortcut':  \n",
    "            prev_layer_id1 = layer_id + int(items['from'])  \n",
    "            prev_layer_id2 = layer_id - 1  \n",
    "            bottom1 = topnames[prev_layer_id1]  \n",
    "            bottom2= topnames[prev_layer_id2]  \n",
    "            shortcut_layer = OrderedDict()  \n",
    "            shortcut_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                shortcut_layer['top'] = items['name']  \n",
    "                shortcut_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                shortcut_layer['top'] = 'layer%d-shortcut' % layer_id  \n",
    "                shortcut_layer['name'] = 'layer%d-shortcut' % layer_id  \n",
    "            shortcut_layer['type'] = 'Eltwise'\n",
    "            eltwise_param = OrderedDict()  \n",
    "            eltwise_param['operation'] = 'SUM'  \n",
    "            shortcut_layer['eltwise_param'] = eltwise_param  \n",
    "            layers.append(shortcut_layer)  \n",
    "            bottom = shortcut_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if block.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1             \n",
    "\n",
    "        elif items['type'] == 'connected':  \n",
    "            fc_layer = OrderedDict()  \n",
    "            fc_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer['top'] = items['name']  \n",
    "                fc_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                fc_layer['top'] = 'layer%d-fc' % layer_id  \n",
    "                fc_layer['name'] = 'layer%d-fc' % layer_id  \n",
    "            fc_layer['type'] = 'InnerProduct'  \n",
    "            fc_param = OrderedDict()  \n",
    "            fc_param['num_output'] = int(items['output'])  \n",
    "            fc_layer['inner_product_param'] = fc_param  \n",
    "            layers.append(fc_layer)  \n",
    "            bottom = fc_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % items['type'])  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "    net_info = OrderedDict()  \n",
    "    net_info['props'] = props  \n",
    "    net_info['layers'] = layers  \n",
    "    return net_info \n",
    "def save_prototxt(net_info, protofile, yolo=False):\n",
    "    fp = open(protofile, 'w')\n",
    "    # whether add double quote\n",
    "    def format_value(value):\n",
    "        #str = u'%s' % value\n",
    "        #if str.isnumeric():\n",
    "        if is_number(value):\n",
    "            return value\n",
    "        elif value == 'true' or value == 'false' or value == 'MAX' or value == 'SUM' or value == 'AVE':\n",
    "            return value\n",
    "        else:\n",
    "            return '\\\"%s\\\"' % value\n",
    "\n",
    "    def print_block(block_info, prefix, indent):\n",
    "        blanks = ''.join([' ']*indent)\n",
    "        print >>fp, '%s%s {' % (blanks, prefix)\n",
    "        for key,value in block_info.items():\n",
    "            if type(value) == OrderedDict:\n",
    "                print_block(value, key, indent+4)\n",
    "            elif type(value) == list:\n",
    "                for v in value:\n",
    "                    print >> fp, '%s    %s: %s' % (blanks, key, format_value(v))\n",
    "            else:\n",
    "                print >> fp, '%s    %s: %s' % (blanks, key, format_value(value))\n",
    "        print >> fp, '%s}' % blanks\n",
    "        \n",
    "    props = net_info['props']\n",
    "    layers = net_info['layers']\n",
    "    print >> fp, 'name: \\\"%s\\\"' % props['name']\n",
    "    print >> fp, 'input: \\\"%s\\\"' % props['input']\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][0]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][1]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][2]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][3]\n",
    "    print >> fp, ''\n",
    "    for layer in layers:\n",
    "        if layer['type'] != 'yolo' or yolo == True:\n",
    "            print_block(layer, 'layer', 0)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfgfile = '/data/github_repos/darknet/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "cfgfile = '/data/darknet/cfg/yolov3.cfg'\n",
    "#cfg2prototxt(cfgfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['net_1', 'convolutional_2', 'convolutional_3', 'convolutional_4', 'convolutional_5', 'shortcut_6', 'convolutional_7', 'convolutional_8', 'convolutional_9', 'shortcut_10', 'convolutional_11', 'convolutional_12', 'shortcut_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'shortcut_17', 'convolutional_18', 'convolutional_19', 'shortcut_20', 'convolutional_21', 'convolutional_22', 'shortcut_23', 'convolutional_24', 'convolutional_25', 'shortcut_26', 'convolutional_27', 'convolutional_28', 'shortcut_29', 'convolutional_30', 'convolutional_31', 'shortcut_32', 'convolutional_33', 'convolutional_34', 'shortcut_35', 'convolutional_36', 'convolutional_37', 'shortcut_38', 'convolutional_39', 'convolutional_40', 'convolutional_41', 'shortcut_42', 'convolutional_43', 'convolutional_44', 'shortcut_45', 'convolutional_46', 'convolutional_47', 'shortcut_48', 'convolutional_49', 'convolutional_50', 'shortcut_51', 'convolutional_52', 'convolutional_53', 'shortcut_54', 'convolutional_55', 'convolutional_56', 'shortcut_57', 'convolutional_58', 'convolutional_59', 'shortcut_60', 'convolutional_61', 'convolutional_62', 'shortcut_63', 'convolutional_64', 'convolutional_65', 'convolutional_66', 'shortcut_67', 'convolutional_68', 'convolutional_69', 'shortcut_70', 'convolutional_71', 'convolutional_72', 'shortcut_73', 'convolutional_74', 'convolutional_75', 'shortcut_76', 'convolutional_77', 'convolutional_78', 'convolutional_79', 'convolutional_80', 'convolutional_81', 'convolutional_82', 'convolutional_83', 'yolo_84', 'route_85', 'convolutional_86', 'upsample_87', 'route_88', 'convolutional_89', 'convolutional_90', 'convolutional_91', 'convolutional_92', 'convolutional_93', 'convolutional_94', 'convolutional_95', 'yolo_96', 'route_97', 'convolutional_98', 'upsample_99', 'route_100', 'convolutional_101', 'convolutional_102', 'convolutional_103', 'convolutional_104', 'convolutional_105', 'convolutional_106', 'convolutional_107', 'yolo_108']\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.001', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '500200', 'steps': '400000,450000', 'type': 'net', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '255', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'random': '1', 'mask': '6,7,8', 'num': '9', 'classes': '80', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n",
      "{'layers': '-4', 'type': 'route'}\n",
      "-4\n",
      "OrderedDict([('bottom', 'layer80-conv'), ('top', 'layer84-route'), ('name', 'layer84-route'), ('type', 'Concat')])\n",
      "84\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'stride': '2', 'type': 'upsample'}\n",
      "2\n",
      "OrderedDict([('bottom', 'layer85-conv'), ('top', 'layer86-upsample'), ('name', 'layer86-upsample'), ('type', 'Upsample'), ('upsample_param', OrderedDict([('scale', '2')]))])\n",
      "('upsample:', 86)\n",
      "{'layers': '-1, 61', 'type': 'route'}\n",
      "-1\n",
      "{1: 'layer1-conv', 2: 'layer2-conv', 3: 'layer3-conv', 4: 'layer4-conv', 5: 'layer5-shortcut', 6: 'layer6-conv', 7: 'layer7-conv', 8: 'layer8-conv', 9: 'layer9-shortcut', 10: 'layer10-conv', 11: 'layer11-conv', 12: 'layer12-shortcut', 13: 'layer13-conv', 14: 'layer14-conv', 15: 'layer15-conv', 16: 'layer16-shortcut', 17: 'layer17-conv', 18: 'layer18-conv', 19: 'layer19-shortcut', 20: 'layer20-conv', 21: 'layer21-conv', 22: 'layer22-shortcut', 23: 'layer23-conv', 24: 'layer24-conv', 25: 'layer25-shortcut', 26: 'layer26-conv', 27: 'layer27-conv', 28: 'layer28-shortcut', 29: 'layer29-conv', 30: 'layer30-conv', 31: 'layer31-shortcut', 32: 'layer32-conv', 33: 'layer33-conv', 34: 'layer34-shortcut', 35: 'layer35-conv', 36: 'layer36-conv', 37: 'layer37-shortcut', 38: 'layer38-conv', 39: 'layer39-conv', 40: 'layer40-conv', 41: 'layer41-shortcut', 42: 'layer42-conv', 43: 'layer43-conv', 44: 'layer44-shortcut', 45: 'layer45-conv', 46: 'layer46-conv', 47: 'layer47-shortcut', 48: 'layer48-conv', 49: 'layer49-conv', 50: 'layer50-shortcut', 51: 'layer51-conv', 52: 'layer52-conv', 53: 'layer53-shortcut', 54: 'layer54-conv', 55: 'layer55-conv', 56: 'layer56-shortcut', 57: 'layer57-conv', 58: 'layer58-conv', 59: 'layer59-shortcut', 60: 'layer60-conv', 61: 'layer61-conv', 62: 'layer62-shortcut', 63: 'layer63-conv', 64: 'layer64-conv', 65: 'layer65-conv', 66: 'layer66-shortcut', 67: 'layer67-conv', 68: 'layer68-conv', 69: 'layer69-shortcut', 70: 'layer70-conv', 71: 'layer71-conv', 72: 'layer72-shortcut', 73: 'layer73-conv', 74: 'layer74-conv', 75: 'layer75-shortcut', 76: 'layer76-conv', 77: 'layer77-conv', 78: 'layer78-conv', 79: 'layer79-conv', 80: 'layer80-conv', 81: 'layer81-conv', 82: 'layer82-conv', 84: 'layer84-route', 85: 'layer85-conv', 86: 'layer86-upsample'}\n",
      "OrderedDict([('bottom', ['layer86-upsample', 'layer62-shortcut']), ('top', 'layer87-route'), ('name', 'layer87-route'), ('type', 'Concat')])\n",
      "87\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '255', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'random': '1', 'mask': '3,4,5', 'num': '9', 'classes': '80', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n",
      "{'layers': '-4', 'type': 'route'}\n",
      "-4\n",
      "OrderedDict([('bottom', 'layer92-conv'), ('top', 'layer96-route'), ('name', 'layer96-route'), ('type', 'Concat')])\n",
      "96\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'stride': '2', 'type': 'upsample'}\n",
      "2\n",
      "OrderedDict([('bottom', 'layer97-conv'), ('top', 'layer98-upsample'), ('name', 'layer98-upsample'), ('type', 'Upsample'), ('upsample_param', OrderedDict([('scale', '2')]))])\n",
      "('upsample:', 98)\n",
      "{'layers': '-1, 36', 'type': 'route'}\n",
      "-1\n",
      "{1: 'layer1-conv', 2: 'layer2-conv', 3: 'layer3-conv', 4: 'layer4-conv', 5: 'layer5-shortcut', 6: 'layer6-conv', 7: 'layer7-conv', 8: 'layer8-conv', 9: 'layer9-shortcut', 10: 'layer10-conv', 11: 'layer11-conv', 12: 'layer12-shortcut', 13: 'layer13-conv', 14: 'layer14-conv', 15: 'layer15-conv', 16: 'layer16-shortcut', 17: 'layer17-conv', 18: 'layer18-conv', 19: 'layer19-shortcut', 20: 'layer20-conv', 21: 'layer21-conv', 22: 'layer22-shortcut', 23: 'layer23-conv', 24: 'layer24-conv', 25: 'layer25-shortcut', 26: 'layer26-conv', 27: 'layer27-conv', 28: 'layer28-shortcut', 29: 'layer29-conv', 30: 'layer30-conv', 31: 'layer31-shortcut', 32: 'layer32-conv', 33: 'layer33-conv', 34: 'layer34-shortcut', 35: 'layer35-conv', 36: 'layer36-conv', 37: 'layer37-shortcut', 38: 'layer38-conv', 39: 'layer39-conv', 40: 'layer40-conv', 41: 'layer41-shortcut', 42: 'layer42-conv', 43: 'layer43-conv', 44: 'layer44-shortcut', 45: 'layer45-conv', 46: 'layer46-conv', 47: 'layer47-shortcut', 48: 'layer48-conv', 49: 'layer49-conv', 50: 'layer50-shortcut', 51: 'layer51-conv', 52: 'layer52-conv', 53: 'layer53-shortcut', 54: 'layer54-conv', 55: 'layer55-conv', 56: 'layer56-shortcut', 57: 'layer57-conv', 58: 'layer58-conv', 59: 'layer59-shortcut', 60: 'layer60-conv', 61: 'layer61-conv', 62: 'layer62-shortcut', 63: 'layer63-conv', 64: 'layer64-conv', 65: 'layer65-conv', 66: 'layer66-shortcut', 67: 'layer67-conv', 68: 'layer68-conv', 69: 'layer69-shortcut', 70: 'layer70-conv', 71: 'layer71-conv', 72: 'layer72-shortcut', 73: 'layer73-conv', 74: 'layer74-conv', 75: 'layer75-shortcut', 76: 'layer76-conv', 77: 'layer77-conv', 78: 'layer78-conv', 79: 'layer79-conv', 80: 'layer80-conv', 81: 'layer81-conv', 82: 'layer82-conv', 84: 'layer84-route', 85: 'layer85-conv', 86: 'layer86-upsample', 87: 'layer87-route', 88: 'layer88-conv', 89: 'layer89-conv', 90: 'layer90-conv', 91: 'layer91-conv', 92: 'layer92-conv', 93: 'layer93-conv', 94: 'layer94-conv', 96: 'layer96-route', 97: 'layer97-conv', 98: 'layer98-upsample'}\n",
      "OrderedDict([('bottom', ['layer98-upsample', 'layer37-shortcut']), ('top', 'layer99-route'), ('name', 'layer99-route'), ('type', 'Concat')])\n",
      "99\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '255', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'random': '1', 'mask': '0,1,2', 'num': '9', 'classes': '80', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    '''\n",
    "    cfgfile = sys.argv[1]   \n",
    "    weightfile = sys.argv[2]  \n",
    "    protofile = sys.argv[3]  \n",
    "    caffemodel = sys.argv[4]\n",
    "    '''\n",
    "    import datetime\n",
    "    \n",
    "    saved_prototxt = str(datetime.datetime.now()).split(' ')[-1] + cfgfile.split('/')[-1] + '.prototxt'\n",
    "    net_info = cfg2prototxt(cfgfile)    \n",
    "    save_prototxt(net_info, saved_prototxt) \n",
    "    #darknet2caffe(cfgfile, weightfile, protofile, caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
